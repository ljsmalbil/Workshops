{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Workshop_I_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSiCMGKeHBsp",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis using Python and Pands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoBiafpmHBst",
        "colab_type": "text"
      },
      "source": [
        "## Preprosessing Textual Data using Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H-_Y887HBsw",
        "colab_type": "text"
      },
      "source": [
        "In this first section, we will have a look at how textual data can be processed by a computer. In order to do so, we will use a tool called Pandas. First, we will convert some textual data into an array. Then, we vectorise it, which means that we transform each sentence into a vector of a length equal to the other vectors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWRwp7ruHBs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Create text\n",
        "text_data = np.array(['I love Paris. Paris!',\n",
        "                      'Japan is best',\n",
        "                      'Germany is lovely',\n",
        "                      'I do like Iceland',\n",
        "                      'I love that movie',\n",
        "                      'Cake is bad for you',\n",
        "                      'Dont go to that restaurant',\n",
        "                       'Very nice pies'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQTIbpeIHBtD",
        "colab_type": "text"
      },
      "source": [
        "Below, we will create a so-called Bag of Words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrwYiZBVHBtH",
        "colab_type": "code",
        "outputId": "eaffa2c2-8442-48a4-99c2-003cd8f7e96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create the bag of words feature matrix\n",
        "count = CountVectorizer()\n",
        "bag_of_words = count.fit_transform(text_data)\n",
        "\n",
        "# Show feature matrix\n",
        "bag_of_words.toarray()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
              "        0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0],\n",
              "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "        0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "        0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmZ5mqaaHBtS",
        "colab_type": "text"
      },
      "source": [
        "If we want, we can inspect the features (in this case the words) and convert them into a pandas table. This allows us to easly inspect the data and see if everyhing's there. Moreoever, we can also add the label (i.e. the positive or negative sentiment label) to this table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLVAsEqcHBtU",
        "colab_type": "code",
        "outputId": "539796fd-859a-4952-fea8-27223302b7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bad',\n",
              " 'best',\n",
              " 'cake',\n",
              " 'do',\n",
              " 'dont',\n",
              " 'for',\n",
              " 'germany',\n",
              " 'go',\n",
              " 'iceland',\n",
              " 'is',\n",
              " 'japan',\n",
              " 'like',\n",
              " 'love',\n",
              " 'lovely',\n",
              " 'movie',\n",
              " 'nice',\n",
              " 'paris',\n",
              " 'pies',\n",
              " 'restaurant',\n",
              " 'that',\n",
              " 'to',\n",
              " 'very',\n",
              " 'you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f65--sPHBtb",
        "colab_type": "code",
        "outputId": "6687e4fd-330e-4d95-d1ad-df3849d57ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "text_data = np.array(['I love Paris. Paris!',\n",
        "                      'Japan is best',\n",
        "                      'Germany is lovely',\n",
        "                      'I dont like Iceland',\n",
        "                      'I love that movie',\n",
        "                      'Cake is bad for you',\n",
        "                      'Dont go to that restaurant',\n",
        "                       'Very nice pies'])\n",
        "\n",
        "# Create data frame\n",
        "data_frame = pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
        "data_frame"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bad</th>\n",
              "      <th>best</th>\n",
              "      <th>cake</th>\n",
              "      <th>do</th>\n",
              "      <th>dont</th>\n",
              "      <th>for</th>\n",
              "      <th>germany</th>\n",
              "      <th>go</th>\n",
              "      <th>iceland</th>\n",
              "      <th>is</th>\n",
              "      <th>japan</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>lovely</th>\n",
              "      <th>movie</th>\n",
              "      <th>nice</th>\n",
              "      <th>paris</th>\n",
              "      <th>pies</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>that</th>\n",
              "      <th>to</th>\n",
              "      <th>very</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bad  best  cake  do  dont  for  ...  pies  restaurant  that  to  very  you\n",
              "0    0     0     0   0     0    0  ...     0           0     0   0     0    0\n",
              "1    0     1     0   0     0    0  ...     0           0     0   0     0    0\n",
              "2    0     0     0   0     0    0  ...     0           0     0   0     0    0\n",
              "3    0     0     0   1     0    0  ...     0           0     0   0     0    0\n",
              "4    0     0     0   0     0    0  ...     0           0     1   0     0    0\n",
              "5    1     0     1   0     0    1  ...     0           0     0   0     0    1\n",
              "6    0     0     0   0     1    0  ...     0           1     1   1     0    0\n",
              "7    0     0     0   0     0    0  ...     1           0     0   0     1    0\n",
              "\n",
              "[8 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adFV2-L9HBti",
        "colab_type": "code",
        "outputId": "9afb849d-960b-416d-fa76-826ba7ddacb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "#Add labels\n",
        "data_frame['Sentiment'] = [1,1,1,0,1,0,0,1]\n",
        "data_frame"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bad</th>\n",
              "      <th>best</th>\n",
              "      <th>cake</th>\n",
              "      <th>do</th>\n",
              "      <th>dont</th>\n",
              "      <th>for</th>\n",
              "      <th>germany</th>\n",
              "      <th>go</th>\n",
              "      <th>iceland</th>\n",
              "      <th>is</th>\n",
              "      <th>japan</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>lovely</th>\n",
              "      <th>movie</th>\n",
              "      <th>nice</th>\n",
              "      <th>paris</th>\n",
              "      <th>pies</th>\n",
              "      <th>restaurant</th>\n",
              "      <th>that</th>\n",
              "      <th>to</th>\n",
              "      <th>very</th>\n",
              "      <th>you</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bad  best  cake  do  dont  for  ...  restaurant  that  to  very  you  Sentiment\n",
              "0    0     0     0   0     0    0  ...           0     0   0     0    0          1\n",
              "1    0     1     0   0     0    0  ...           0     0   0     0    0          1\n",
              "2    0     0     0   0     0    0  ...           0     0   0     0    0          1\n",
              "3    0     0     0   1     0    0  ...           0     0   0     0    0          0\n",
              "4    0     0     0   0     0    0  ...           0     1   0     0    0          1\n",
              "5    1     0     1   0     0    1  ...           0     0   0     0    1          0\n",
              "6    0     0     0   0     1    0  ...           1     1   1     0    0          0\n",
              "7    0     0     0   0     0    0  ...           0     0   0     1    0          1\n",
              "\n",
              "[8 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8tP45iYHBtm",
        "colab_type": "text"
      },
      "source": [
        "## Here we will start with the machine learning data-preprosessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQyCD2MKHBtn",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will work on the implementation of the machine learning algorithm. This is just a 'toy' example on what this would look like as in reality we would need much more data. In a later section, we will do so, but for now it is good to get some feeling on what implementing such a network might look like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb3wnu49HBtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN5_TL2KHBtt",
        "colab_type": "text"
      },
      "source": [
        "First, we will split the table into two: a training set and a test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dFZ20VFHBtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = data_frame['Sentiment']\n",
        "data = data_frame.drop('Sentiment',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DiotWvuHBtx",
        "colab_type": "code",
        "outputId": "8e834d2e-d78e-4965-8bfe-48299c42f885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the shape of the data frame\n",
        "data_frame.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g_V9nWQHBt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From 'regular data' to a test and training set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhWyUQP8HBt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalisation\n",
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsb9omOlHBt7",
        "colab_type": "code",
        "outputId": "4041f7a6-26a9-4595-b104-6aa9c581bea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fit only to the training data\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q11zXXdpHBt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From pdataframe to numpy_array \n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMXDwjaYHBuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJbJcXVbHBuI",
        "colab_type": "text"
      },
      "source": [
        "## Training the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cphTNqqpHBuJ",
        "colab_type": "text"
      },
      "source": [
        "After preprocessing, the first step is to feed the preprossed data to a machine learing algorithm and let it train on the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASpcVcYKHBuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osP7DXrKHBuM",
        "colab_type": "text"
      },
      "source": [
        "We define a model and store that model (by assigning it to a variables) in order to work with it later one. Moreoever, it allows us to easily compare it to other models and prevents it from training the same model more than once. The model below is a very simple model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VD_9XKnHBuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYqBQWe4HBuS",
        "colab_type": "text"
      },
      "source": [
        "Having defined the model, we fit the data to it. This is the 'actual' feeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfFzOd62HBuT",
        "colab_type": "code",
        "outputId": "5530332c-1f50-4e9c-ad86-f19f495f4fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Fit the data \n",
        "mlp.fit(X_train,y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQsxVNwVHBuX",
        "colab_type": "text"
      },
      "source": [
        "## Training and Evaluating the Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N6GTIppHBuX",
        "colab_type": "text"
      },
      "source": [
        "Having trained a machine learning algorithm is one thing. Obviously, we want to evaluate its performance, because without it, we cannot say whether or not it performs well on unseen examples. We test the performance of the model using the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H9Cr86WHBuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = mlp.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxiQVls6HBub",
        "colab_type": "code",
        "outputId": "94f0e689-ecb1-40f2-bcbd-50d2d0145c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(confusion_matrix(y_test,predictions))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7dwjkubHBuf",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can print the scores of the performance and see how our model did. In this case, these do not make sense, as we only input very little data to the algorithm. Still, it is good to get some feeling as to how evaluation works and what we evaluate on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJOVSX-qHBuh",
        "colab_type": "code",
        "outputId": "eef59ae8-87f9-4ae1-e08c-f3e114f08d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       2.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iYtjuPhHBuk",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDSpfMPFHBul",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will use a sufficiently large data set, the IMDB review data set. Moreoever, we will use a more complex model. However, the procedure is the same as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBfn9Tq2HBum",
        "colab_type": "code",
        "outputId": "47b408f1-a726-48a5-92cf-e16953701235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils, to_categorical\n",
        " \n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cipVjY3BHBuo",
        "colab_type": "text"
      },
      "source": [
        "Once again, we import the data set and immmediatly split it into a test and a training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSnwmOy9HBup",
        "colab_type": "code",
        "outputId": "e694ce53-3f45-455b-a56f-00fd0a8ff483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Use only the first 10.000 words\n",
        "NUM_WORDS=10000 \n",
        "INDEX_FROM=3  \n",
        "\n",
        "train,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
        "train_x,train_y = train\n",
        "test_x,test_y = test\n",
        "\n",
        "#print(test_x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-aOTQJSHBuq",
        "colab_type": "text"
      },
      "source": [
        "The code below consists of a function that can help you to see what the reviews in the data base are like. The imported data base already contains encoded (i.e. Bag of Words) data, but we can decode it into readable sentences using this function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwCyBMr-HBur",
        "colab_type": "code",
        "outputId": "4f2836a5-b35a-44cf-9199-0fa580664fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#Takes as an input an i, the ID number of the review and outputs the review as a string. \n",
        "\n",
        "def id_to_word_funct(i):\n",
        "    word_to_id = keras.datasets.imdb.get_word_index()\n",
        "    word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "    word = ' '.join(id_to_word[id] for id in train_x[i])\n",
        "    return word\n",
        "\n",
        "#1 is positive\n",
        "#0 is negative\n",
        "id_to_word_funct(10)\n",
        "#print(train_y[7])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qamiv3HBuw",
        "colab_type": "text"
      },
      "source": [
        "As before, we have to make a feature vector. Moreoever, because every vector has to be of the same length, we need to add so-called padding. Padding simply means that the empty places in the data set are being replaced by 0's.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGbVQxy3HBux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add padding to convert the data points to the same size\n",
        "train_x_seq = keras.preprocessing.sequence.pad_sequences(train_x, maxlen=256, padding='pre', value=0.0)\n",
        "test_x_seq = keras.preprocessing.sequence.pad_sequences(test_x, maxlen=256, padding='pre', value=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpwUGr4AHBuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type(NUM_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9iMwHPrHBu0",
        "colab_type": "text"
      },
      "source": [
        "We can now define our model, as we did before. Them model is a bit more complicated, but the idea is the same: we define a model and add layers to it. Because this model has many layers, we call it a deep network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhhmK3cNHBu1",
        "colab_type": "code",
        "outputId": "f40c2252-df19-4a53-c2c6-560c52170d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
        "vocab_size = NUM_WORDS\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT9Wshe6HBu3",
        "colab_type": "code",
        "outputId": "31429998-f47e-4c86-d93c-eba86123687e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the length \n",
        "print(len(train_x_seq[0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyyFZQmOHBu6",
        "colab_type": "text"
      },
      "source": [
        "Here we compile the model. This means that we determine a loss function and a metric which will be used to test the model. In the case we choose accuracy (acc.). Hence, we want our model to be as accurate as possible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zljz171NHBu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlufOXnKHBu9",
        "colab_type": "text"
      },
      "source": [
        "Below is an intermediate step which we haven't encountered before: validation. Instead of training an algorithm and then testing it, we can also let it make predictions on unlabelled instance while training. This increases the model's performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cc7VpCwHBu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create validation set\n",
        "x_val = train_x_seq[:10000]\n",
        "partial_x_train = train_x_seq[10000:]\n",
        "\n",
        "y_val = train_y[:10000]\n",
        "partial_y_train = train_y[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYlhrJ-gHBu_",
        "colab_type": "code",
        "outputId": "fe5f8bf1-9af0-4310-efbb-e9e22ed8816f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 50us/step - loss: 0.6920 - acc: 0.4996 - val_loss: 0.6897 - val_acc: 0.5542\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.6863 - acc: 0.6339 - val_loss: 0.6820 - val_acc: 0.6324\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.6753 - acc: 0.6758 - val_loss: 0.6684 - val_acc: 0.6975\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.6569 - acc: 0.7315 - val_loss: 0.6473 - val_acc: 0.7321\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.6299 - acc: 0.7641 - val_loss: 0.6187 - val_acc: 0.7738\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5948 - acc: 0.7989 - val_loss: 0.5835 - val_acc: 0.7948\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.5540 - acc: 0.8241 - val_loss: 0.5445 - val_acc: 0.8197\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5096 - acc: 0.8466 - val_loss: 0.5039 - val_acc: 0.8281\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.4651 - acc: 0.8575 - val_loss: 0.4643 - val_acc: 0.8451\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.4212 - acc: 0.8723 - val_loss: 0.4283 - val_acc: 0.8513\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.3827 - acc: 0.8798 - val_loss: 0.3982 - val_acc: 0.8588\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.3505 - acc: 0.8882 - val_loss: 0.3747 - val_acc: 0.8629\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.3240 - acc: 0.8938 - val_loss: 0.3557 - val_acc: 0.8681\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.3020 - acc: 0.8999 - val_loss: 0.3408 - val_acc: 0.8715\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2824 - acc: 0.9037 - val_loss: 0.3293 - val_acc: 0.8727\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2659 - acc: 0.9095 - val_loss: 0.3202 - val_acc: 0.8746\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2515 - acc: 0.9134 - val_loss: 0.3112 - val_acc: 0.8791\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2376 - acc: 0.9193 - val_loss: 0.3049 - val_acc: 0.8811\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.2256 - acc: 0.9229 - val_loss: 0.2996 - val_acc: 0.8824\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.2147 - acc: 0.9268 - val_loss: 0.2953 - val_acc: 0.8830\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2046 - acc: 0.9308 - val_loss: 0.2920 - val_acc: 0.8834\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1959 - acc: 0.9345 - val_loss: 0.2890 - val_acc: 0.8841\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1870 - acc: 0.9385 - val_loss: 0.2875 - val_acc: 0.8850\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1792 - acc: 0.9415 - val_loss: 0.2865 - val_acc: 0.8842\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1713 - acc: 0.9449 - val_loss: 0.2845 - val_acc: 0.8856\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1642 - acc: 0.9483 - val_loss: 0.2851 - val_acc: 0.8843\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1581 - acc: 0.9508 - val_loss: 0.2838 - val_acc: 0.8869\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1513 - acc: 0.9543 - val_loss: 0.2842 - val_acc: 0.8865\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1455 - acc: 0.9560 - val_loss: 0.2849 - val_acc: 0.8854\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1397 - acc: 0.9584 - val_loss: 0.2859 - val_acc: 0.8855\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1345 - acc: 0.9600 - val_loss: 0.2871 - val_acc: 0.8870\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1293 - acc: 0.9621 - val_loss: 0.2881 - val_acc: 0.8862\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1247 - acc: 0.9643 - val_loss: 0.2907 - val_acc: 0.8845\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1197 - acc: 0.9669 - val_loss: 0.2918 - val_acc: 0.8853\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1153 - acc: 0.9681 - val_loss: 0.2939 - val_acc: 0.8855\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1118 - acc: 0.9690 - val_loss: 0.2973 - val_acc: 0.8845\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1071 - acc: 0.9709 - val_loss: 0.2990 - val_acc: 0.8853\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.1033 - acc: 0.9721 - val_loss: 0.3014 - val_acc: 0.8836\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.0997 - acc: 0.9745 - val_loss: 0.3051 - val_acc: 0.8835\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0958 - acc: 0.9755 - val_loss: 0.3080 - val_acc: 0.8829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j51ieJGgHBvC",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can test our model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGjIqrToHBvD",
        "colab_type": "code",
        "outputId": "ca81025b-b2d6-4059-cf8e-222963a89cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(test_x_seq, test_y)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 30us/step\n",
            "[0.3272354226446152, 0.8731600046157837]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4zZqy4pHBvF",
        "colab_type": "text"
      },
      "source": [
        "# Let's write and predict our own review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGGHLHoxHBvF",
        "colab_type": "text"
      },
      "source": [
        "It seems that our model has an accuracy of somewhere between 0.85 and 0.90. This means that in about 90% of the cases, the test data was classified correctly. However, this is all nice and good, but we would like to see how this works in real time. For this reason, please write your own movie review (negative or positive) and see whether the prediction the model makes is correct or not. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8WS5ajzHBvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "\n",
        "#review = 'Then youll like also the movie! Of course if you see only the movie probably youll say this is a boring movie without an intense screenplay! And probably youll be right! But this is not a movie! This is a celebration of downton abbey and the characters weve loved! You will feel nostalgia! So if you want to see an extra long episode of your favorite tv show go to the cinema!'\n",
        "#review = 'Saw a preview tonight, which thankfully gave a catch up or refresher which may have gone way too fast for someone who had never seen the series. It was a lovely film, costumes and cinematography worthy, but the storyline was dull, and the acting predictable. Prediction: no awards but a moneymaker'\n",
        "#review = 'Sorry folks but this movie is a C movie and not an A. I cant believe that people like this movie so much. Yes I am biased, I really dont like Tim Robbins but he has made a decent movie or two but this aint one of them. This is your basic prison movie. What can I say it has the host of usual characters. The storyline is basic, the outcome I wont say but it is just stupid to say that this is a great movie. I didnt think it was realistic at all and anybody who falls for this crap is either delusional or needs some brains to differentiate between an Edsel and a Lamborghini. I wouldt watch this movie again unless I was paid to do so. One viewing is enough for this predictable laugher.'\n",
        "#review = 'It is one of the best movies in the world that many people will join me. Watching very, very impressed. What it might be called the great stories of Stephen King anyway. Acting on cues can say is enormous. Theres no posturing. Especially when Andy Dufrense incredible music that review still stands wandering on everyones lips. Finale as you lose yourself in it is a movie you are getting into. Finals if he so exhilarating you a high-quality music. And finding that deserves really deserve. I watched this film very young age and it has taught me that the magic of cinema in a nice way. This film is one of the reasons I hate the Oscars. Will remain as the best film in more than 10 years, I believe wholeheartedly sites. Definitely one of the best films in the world.'\n",
        "review = 'this is the best movie ive ever seen. my friend suggested it to me, to see it for 3 months and I finally did and Im absolutely happyt that I did.'\n",
        "\n",
        "list_of_words = re.sub(\"[^\\w]\", \" \",  review).split()\n",
        "new_review = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Snr0I6gHBvH",
        "colab_type": "code",
        "outputId": "2d0ad86f-b0c3-4d45-81f3-beca720c8a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(list_of_words)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'is', 'the', 'best', 'movie', 'ive', 'ever', 'seen', 'my', 'friend', 'suggested', 'it', 'to', 'me', 'to', 'see', 'it', 'for', '3', 'months', 'and', 'I', 'finally', 'did', 'and', 'Im', 'absolutely', 'happyt', 'that', 'I', 'did']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fA57DZ0HBvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The function below takes as an input a string of one or more words. This string is being converted into a \n",
        "list of strings (i.e. words). Each of them is compared to the values in the word_index of the ibdm DB. \n",
        "Only NUM_WORDS are compared (usually 10.000). Then, a np.array is created from all of these values. This array\n",
        "is padded and passed to the model in order to predict the sentiment. This sentiment will be the output. \n",
        "\n",
        "Below are some examples of reviews. \n",
        "\n",
        "'''\n",
        "\n",
        "def prediction_review(review,NUM_WORDS=10000, model=model):\n",
        "    word_to_id = keras.datasets.imdb.get_word_index()\n",
        "    word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "    \n",
        "    wordList = re.sub(\"[^\\w]\", \" \",  review).split()\n",
        "    new_review = []\n",
        "\n",
        "    for word in wordList:\n",
        "        try:\n",
        "            value = word_to_id[word]\n",
        "            if value <= NUM_WORDS:\n",
        "                new_review.append(word_to_id[word])\n",
        "            else:\n",
        "                continue\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "    new_array = []\n",
        "    new_array.append(new_review)\n",
        "    npa = np.asarray(new_array, dtype=np.float32) \n",
        "\n",
        "    try_out = keras.preprocessing.sequence.pad_sequences(npa, maxlen=256, padding='pre', value=0.0)\n",
        "    prediction = model.predict_classes(try_out) \n",
        "    \n",
        "    prediction = str(prediction)\n",
        "    \n",
        "    if '1' in prediction:\n",
        "        return('The predicted sentiment in this review is positive.')\n",
        "    elif '0' in prediction:\n",
        "        return('The predicted sentiment in this review is negative.')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGoK38AaHBvL",
        "colab_type": "code",
        "outputId": "f6c3f925-cc07-48dd-c937-30c1fe082aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re \n",
        "\n",
        "#review = 'Then youll like also the movie! Of course if you see only the movie probably youll say this is a boring movie without an intense screenplay! And probably youll be right! But this is not a movie! This is a celebration of downton abbey and the characters weve loved! You will feel nostalgia! So if you want to see an extra long episode of your favorite tv show go to the cinema!'\n",
        "#review = 'Saw a preview tonight, which thankfully gave a catch up or refresher which may have gone way too fast for someone who had never seen the series. It was a lovely film, costumes and cinematography worthy, but the storyline was dull, and the acting predictable. Prediction: no awards but a moneymaker'\n",
        "#review = 'Sorry folks but this movie is a C movie and not an A. I cant believe that people like this movie so much. Yes I am biased, I really dont like Tim Robbins but he has made a decent movie or two but this aint one of them. This is your basic prison movie. What can I say it has the host of usual characters. The storyline is basic, the outcome I wont say but it is just stupid to say that this is a great movie. I didnt think it was realistic at all and anybody who falls for this crap is either delusional or needs some brains to differentiate between an Edsel and a Lamborghini. I wouldt watch this movie again unless I was paid to do so. One viewing is enough for this predictable laugher.'\n",
        "#review = 'It is one of the best movies in the world that many people will join me. Watching very, very impressed. What it might be called the great stories of Stephen King anyway. Acting on cues can say is enormous. Theres no posturing. Especially when Andy Dufrense incredible music that review still stands wandering on everyones lips. Finale as you lose yourself in it is a movie you are getting into. Finals if he so exhilarating you a high-quality music. And finding that deserves really deserve. I watched this film very young age and it has taught me that the magic of cinema in a nice way. This film is one of the reasons I hate the Oscars. Will remain as the best film in more than 10 years, I believe wholeheartedly sites. Definitely one of the best films in the world.'\n",
        "#review = 'this is the best movie ive ever seen. my friend suggested it to me, to see it for 3 months and I finally did and Im absolutely happyt that I did.'\n",
        "#review = 'I would rather get a root canal threatment than watch this movie again.'\n",
        "\n",
        "prediction_review(review)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The predicted sentiment in this review is positive.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ3hDZNLo2Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xshFekwN6Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}